{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15_RL_02_TF-Agents.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TN8mzJHjSuc8",
        "9yGS9S-wOElF",
        "s1bTyGUmOElF",
        "TOZLz4jHUopG",
        "I58nZoWhOElH"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfz5KPhrSouT"
      },
      "source": [
        "##라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olUxyFCSQF2f",
        "outputId": "c6a9119d-be42-4403-c4ee-33d99067be4f"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "\n",
        "print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to Runtime > Change runtime and select a GPU hardware accelerator.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hmb4s3KRNkw"
      },
      "source": [
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-u9216eGolE"
      },
      "source": [
        "랜덤 시드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APFiocsZQL-f"
      },
      "source": [
        "seed = 21\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIM-OwIrPqYs"
      },
      "source": [
        "## 코랩 세팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBuBzFuVPnSo"
      },
      "source": [
        "!apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
        "!pip install -q -U tf-agents pyvirtualdisplay gym[atari,box2d]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9K-O4JwMKsS"
      },
      "source": [
        "err: rom is missing에 대한 해결 방법"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPlMOqd2LvXT"
      },
      "source": [
        "! wget http://www.atarimania.com/roms/Roms.rar\n",
        "! mkdir /content/ROM/\n",
        "! unrar e /content/Roms.rar /content/ROM/\n",
        "! python -m atari_py.import_roms /content/ROM/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlprdBJFGt2J"
      },
      "source": [
        "## matplot 세팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "102raf51QPJv"
      },
      "source": [
        "#figure을 예쁘게\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14) #그래프 포함 틀(cf.figure ) axes)\n",
        "mpl.rc('xtick', labelsize=12) #틱은 눈금을 의미\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARzTymgTQR4y"
      },
      "source": [
        "#부드러운 애니메이션\n",
        "import matplotlib.animation as animation\n",
        "mpl.rc('animation', html='jshtml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B5ID2Bvn3sw"
      },
      "source": [
        "# save_fig\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"rl\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR_yIzLNPxlJ"
      },
      "source": [
        "##matplot으로 애니메이션 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXvPp4MdQnSz"
      },
      "source": [
        "한 episode를 실행하고 에니메이션을 위한 frames를 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIsDcj1CeAUe"
      },
      "source": [
        "def render_policy_net(model, n_max_steps=200, random_seed=seed):\n",
        "    frames = []\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    obs = env.reset()\n",
        "    for step in range(n_max_steps):\n",
        "        frames.append(env.render(mode=\"rgb_array\"))\n",
        "        left_proba = model.predict(obs.reshape(1, -1))\n",
        "        action = int(np.random.rand() > left_proba)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "        if done:\n",
        "            break\n",
        "    env.close()\n",
        "    return frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMOGNQi7QyCP"
      },
      "source": [
        "render_policy_net 함수에서 반환받은 frames를 인자로 받아 애니메이션 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D83kyDWlfjJ0"
      },
      "source": [
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()\n",
        "    return anim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN8mzJHjSuc8"
      },
      "source": [
        "##시뮬레이션 환경: TF-Agents의 gym[atari] \"Breakout-v4\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wv-BnnMOEk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2745bd20-91e1-4b9a-81e9-e744e4416aac"
      },
      "source": [
        "from tf_agents.environments import suite_gym\n",
        "\n",
        "env = suite_gym.load(\"Breakout-v4\")\n",
        "env"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_agents.environments.wrappers.TimeLimit at 0x7ff67b723490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrvYPtLIQ_qw"
      },
      "source": [
        "try:\n",
        "    import pyvirtualdisplay\n",
        "    display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
        "except ImportError:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k4PikE7OEk-"
      },
      "source": [
        "# TF-Agents Environments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBJmE2a5OEk_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "227d7ac7-7d0b-4bbf-c6b6-e9e33628a101"
      },
      "source": [
        "env.gym"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gym.envs.atari.atari_env.AtariEnv at 0x7f277f79bd90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "andIqHesOEk_"
      },
      "source": [
        "env.seed(seed)\n",
        "env.reset() #직접 관측을 반환하지는 않는다. 관측과 부가 정보를 감싼 TimeStep 객체 반환"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwdQ5PxhOEk_"
      },
      "source": [
        "# step 매서도도 TimeStep 객체 반환\n",
        "env.step(1) # Fire"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy36AUezOEk_"
      },
      "source": [
        "img = env.render(mode=\"rgb_array\")\n",
        "\n",
        "plt.figure(figsize=(6, 8))\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "save_fig(\"breakout_plot\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7Lja4W2OElA"
      },
      "source": [
        "env.current_time_step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXmxcNm3OElA"
      },
      "source": [
        "# 환경 스펙(Environment Specifications)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3Bnz8ZHOElA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80480444-132f-40a5-d707-7ab69c41d29a"
      },
      "source": [
        "env.observation_spec()\n",
        "# shape=(210, 160, 3) : 단순 아타리 게임 화면의 스크린샷\n",
        "# env.render(mode=\"human\")을 호출하여 환경을 그릴 수 있다\n",
        "# env.render(mode=\"rgb_array\")를 호출하여 넘파이 배열 형태로 이미지를 받을 수 있다"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-sgCgItOElA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f2a1c9-f109-400f-e18d-b97262c785e2"
      },
      "source": [
        "env.action_spec()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edahBJGYOElA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3689ed6-e013-44ec-8339-376006e067ec"
      },
      "source": [
        "env.time_step_spec()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TimeStep(\n",
              "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
              " 'observation': BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255),\n",
              " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
              " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZXyQtqtVVeD"
      },
      "source": [
        "env.render(mode=\"human\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOcavFmMVYTj"
      },
      "source": [
        "env.render(mode=\"rgb_array\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MYNtHc5VZ7n"
      },
      "source": [
        "env.gym.get_action_meanings()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08qoonrPOElB"
      },
      "source": [
        "# Environment Wrappers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmFVMibIOElB"
      },
      "source": [
        "You can wrap a TF-Agents environments in a TF-Agents wrapper:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at218kXwOElB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa45d96-a005-4ee3-ee7c-dd96a1d2c3eb"
      },
      "source": [
        "from tf_agents.environments.wrappers import ActionRepeat\n",
        "\n",
        "repeating_env = ActionRepeat(env, times=4) #행동을 time=n번 반복하면서 보상을 누적. 많은 환경에서 훈련 속도를 크게 높힐 수 있다.\n",
        "repeating_env"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_agents.environments.wrappers.ActionRepeat at 0x7f2774847950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5SmQ7NbOElB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8fcd4ca-7ae0-4a61-9db1-825745f606e4"
      },
      "source": [
        "repeating_env.unwrapped"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gym.envs.atari.atari_env.AtariEnv at 0x7f277f79bd90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYBsADQ3OElB"
      },
      "source": [
        "Here is the list of available wrappers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ABbZ-RkOElC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2152a30a-c1da-49d0-c948-b1047297d446"
      },
      "source": [
        "import tf_agents.environments.wrappers\n",
        "\n",
        "#가능한 래퍼 종류\n",
        "for name in dir(tf_agents.environments.wrappers):\n",
        "    obj = getattr(tf_agents.environments.wrappers, name)\n",
        "    if hasattr(obj, \"__base__\") and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n",
        "        print(\"{:27s} {}\".format(name, obj.__doc__.split(\"\\n\")[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ActionClipWrapper           Wraps an environment and clips actions to spec before applying.\n",
            "ActionDiscretizeWrapper     Wraps an environment with continuous actions and discretizes them.\n",
            "ActionOffsetWrapper         Offsets actions to be zero-based.\n",
            "ActionRepeat                Repeates actions over n-steps while acummulating the received reward.\n",
            "ExtraDisabledActionsWrapper Adds extra unavailable actions.\n",
            "FlattenObservationsWrapper  Wraps an environment and flattens nested multi-dimensional observations.\n",
            "GoalReplayEnvWrapper        Adds a goal to the observation, used for HER (Hindsight Experience Replay).\n",
            "HistoryWrapper              Adds observation and action history to the environment's observations.\n",
            "ObservationFilterWrapper    Filters observations based on an array of indexes.\n",
            "OneHotActionWrapper         Converts discrete action to one_hot format.\n",
            "PerformanceProfiler         End episodes after specified number of steps.\n",
            "PyEnvironmentBaseWrapper    PyEnvironment wrapper forwards calls to the given environment.\n",
            "RunStats                    Wrapper that accumulates run statistics as the environment iterates.\n",
            "TimeLimit                   End episodes after specified number of steps.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg60NmOnOElC"
      },
      "source": [
        "The `suite_gym.load()` function can create an env and wrap it for you, both with TF-Agents environment wrappers and Gym environment wrappers (the latter are applied first)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9rarB9POElC"
      },
      "source": [
        "from functools import partial\n",
        "from gym.wrappers import TimeLimit\n",
        "\n",
        "# 래핑하기\n",
        "limited_repeating_env = suite_gym.load(\n",
        "    \"Breakout-v4\",\n",
        "    gym_env_wrappers=[partial(TimeLimit, max_episode_steps=10000)],\n",
        "    env_wrappers=[partial(ActionRepeat, times=4)],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CM71N1LOElC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec38c946-bf39-442c-92b9-81788d6c6b9b"
      },
      "source": [
        "limited_repeating_env"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_agents.environments.wrappers.ActionRepeat at 0x7f278a35edd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OczlEHH3OElC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b211719b-546c-434e-8992-e55e14568d56"
      },
      "source": [
        "limited_repeating_env.unwrapped"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gym.envs.atari.atari_env.AtariEnv at 0x7f277d0a9c10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N16Z-Ah0OElD"
      },
      "source": [
        "Create an Atari Breakout environment, and wrap it to apply the default Atari preprocessing steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y13UFXbOOElD"
      },
      "source": [
        "**Warning**: Breakout requires the player to press the FIRE button at the start of the game and after each life lost. The agent may take a very long time learning this because at first it seems that pressing FIRE just means losing faster. To speed up training considerably, we create and use a subclass of the `AtariPreprocessing` wrapper class called `AtariPreprocessingWithAutoFire` which presses FIRE (i.e., plays action 1) automatically at the start of the game and after each life lost. This is different from the book which uses the regular `AtariPreprocessing` wrapper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbmgOdKkOElD"
      },
      "source": [
        "from tf_agents.environments import suite_atari\n",
        "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
        "from tf_agents.environments.atari_wrappers import FrameStack4\n",
        "\n",
        "max_episode_steps = 27000 # <=> 108k ALE frames since 1 step = 4 frames(1 스텝 = 4 프레임 이므로 108,000개 ALE 프레임)\n",
        "environment_name = \"BreakoutNoFrameskip-v4\"\n",
        "\n",
        "# AtariPreprocessing: 아타리 환경에서 표준적인 전처리\n",
        "class AtariPreprocessingWithAutoFire(AtariPreprocessing):\n",
        "    def reset(self, **kwargs):\n",
        "        obs = super().reset(**kwargs)\n",
        "        super().step(1) # FIRE to start\n",
        "        return obs\n",
        "    def step(self, action):\n",
        "        lives_before_action = self.ale.lives()\n",
        "        obs, rewards, done, info = super().step(action)\n",
        "        if self.ale.lives() < lives_before_action and not done:\n",
        "            super().step(1) # FIRE to start after life lost\n",
        "        return obs, rewards, done, info\n",
        "\n",
        "env = suite_atari.load(\n",
        "    environment_name,\n",
        "    max_episode_steps=max_episode_steps,\n",
        "    gym_env_wrappers=[AtariPreprocessingWithAutoFire, FrameStack4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "504KcsP9OElD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2759a0b1-e2be-46ef-e740-7af9ffa286bb"
      },
      "source": [
        "env"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_agents.environments.atari_wrappers.AtariTimeLimit at 0x7f277d0c59d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbGOnLg9OElD"
      },
      "source": [
        "Play a few steps just to see what happens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_XRteueOElE"
      },
      "source": [
        "env.seed(42)\n",
        "env.reset()\n",
        "for _ in range(4):\n",
        "    time_step = env.step(3) # LEFT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlrW--vOOElE"
      },
      "source": [
        "def plot_observation(obs):\n",
        "    # Since there are only 3 color channels, you cannot display 4 frames\n",
        "    # with one primary color per frame. So this code computes the delta between\n",
        "    # the current frame and the mean of the other frames, and it adds this delta\n",
        "    # to the red and blue channels to get a pink color for the current frame.\n",
        "    obs = obs.astype(np.float32)\n",
        "    img = obs[..., :3]\n",
        "    current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n",
        "    img[..., 0] += current_frame_delta\n",
        "    img[..., 2] += current_frame_delta\n",
        "    img = np.clip(img / 150, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHYY-M58OElE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "3ddf5511-a4b0-4fdd-ccb2-c11eec42aeb4"
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plot_observation(time_step.observation)\n",
        "save_fig(\"preprocessed_breakout_plot\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving figure preprocessed_breakout_plot\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_02NpZJSOElE"
      },
      "source": [
        "Convert the Python environment to a TF environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kLUDOu0OElE"
      },
      "source": [
        "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
        "\n",
        "# TFPyEnvironment 안에 이 환경 감싸기\n",
        "tf_env = TFPyEnvironment(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzxVI7IAOElF"
      },
      "source": [
        "# Creating the DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYVaVBtQOElF"
      },
      "source": [
        "Create a small class to normalize the observations. Images are stored using bytes from 0 to 255 to use less RAM, but we want to pass floats from 0.0 to 1.0 to the neural network:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yGS9S-wOElF"
      },
      "source": [
        "## Q-Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be_9XTO8GlMV"
      },
      "source": [
        "QNetwork 클래스: 옵저버를 입력으로 받고 행동마다 하나의 Q-가치를 출력.<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmC3Gd4JOElF"
      },
      "source": [
        "from tf_agents.networks.q_network import QNetwork\n",
        "\n",
        "# 관측을 32비트 실수로 변환하고 정규화한다. 이 관측은 32비트 실수보다 4배나 작은 공간을 차지하는 부호 없는 바이트\n",
        "preprocessing_layer = keras.layers.Lambda(\n",
        "                          lambda obs: tf.cast(obs, np.float32) / 255.)\n",
        "# 층이 3개인 CNN\n",
        "# 1st: 8x8 필터 32개와 스트라이드 4\n",
        "# 2nd: 4x4 필터 32개와 스트라이드 32\n",
        "# 3rd: 8x8 필터 64개와 스트라이드 1\n",
        "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
        "\n",
        "# 뉴런 512개를 가진 fc layer\n",
        "fc_layer_params=[512]\n",
        "\n",
        "q_net = QNetwork(\n",
        "    tf_env.observation_spec(), # 관측의 스펙\n",
        "    tf_env.action_spec(), # 행동의 스펙\n",
        "\n",
        "    preprocessing_layers=preprocessing_layer, #관측을 32비트 실수로 변환하고 정규화하는 층\n",
        "    conv_layer_params=conv_layer_params, # 3개 층을 가진 CNN\n",
        "    fc_layer_params=fc_layer_params) # fc layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1bTyGUmOElF"
      },
      "source": [
        "## DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkNEC6GUOElF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4181a6d2-2cd5-492f-954e-eecd67614218"
      },
      "source": [
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "\n",
        "train_step = tf.Variable(0) # 훈련 스텝 횟수\n",
        "update_period = 4 # run a training step every 4 collect steps(4 스텝마다 모델을 훈련)\n",
        "optimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0, epsilon=0.00001, centered=True) # 옵티마이저(2015년 DQN 논문)\n",
        "\n",
        "# ε-그리디 정책을 위한 ε값 계산을 위한 PolynomialDecay 객체\n",
        "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=1.0, # initial ε(초기 ε)\n",
        "    \n",
        "    # 100만 ALE프레임 동안 1.0에서 0.01로 감소(2015년 DQN 논문)\n",
        "    # 4 프레임마다 사용하므로 250,000 스텝에 해당\n",
        "    # 에이전트가 4 스텝(16 ALE 프레임)마다 훈련되므로 ε은 62,500 훈련 스템에 걸쳐 감쇠\n",
        "    decay_steps=250000 // update_period, # <=> 1,000,000 ALE frames(<=> 1,000,000 ALE 프레임)\n",
        "    \n",
        "    end_learning_rate=0.01) # final ε(마지막 ε)\n",
        "\n",
        "# DQNAgent\n",
        "agent = DqnAgent(\n",
        "    tf_env.time_step_spec(), # 타임 스텝 스펙\n",
        "    tf_env.action_spec(), # 행동 스펙\n",
        "    q_network=q_net, # 훈련할 QNetwork\n",
        "    optimizer=optimizer, # 옵티마이저\n",
        "    target_update_period=2000, # 타깃 모델을 업데이트할 훈련 스텝 간격 <=> 32,000 ALE frames\n",
        "    td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"), # 손실 함수\n",
        "    gamma=0.99, # discount factor(할인 계수)\n",
        "    train_step_counter=train_step, # train_step 변수\n",
        "    epsilon_greedy=lambda: epsilon_fn(train_step)) # ε값을 반환하는 함수(이 함수는 매개변수가 없어야 하므로 train_step 변수를 전달하기 위해 lambda 사용)\n",
        "\n",
        "# 에이전트 초기화\n",
        "agent.initialize()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOZLz4jHUopG"
      },
      "source": [
        "## Replay Bufffer & Observer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apcXWcaGOElG"
      },
      "source": [
        "this will use a lot of RAM, so please reduce the buffer size if you get an out-of-memory error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CGnA7RPOElG"
      },
      "source": [
        "**Warning**: we use a replay buffer of size 100,000 instead of 1,000,000 (as used in the book) since many people were getting OOM (Out-Of-Memory) errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6uPKdoLOElG"
      },
      "source": [
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "\n",
        "# 균등 샘플링을 수행하는 고성능 재생 버퍼\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec, # 데이터 스펙: DQN 에이전트는 수집된 데이터의 구조를 알고 collect_data_spec속성을 참조할 수 있다)\n",
        "    batch_size=tf_env.batch_size, # 각 스텝에서 추가될 경로의 개수(드라이버가 스텝마다 하나의 행동을 실행하고 하나의 경로를 수집하므로 이 값은 1)\n",
        "    max_length=100000) # 재생 버터의 최대 크기(2015년 DQN 논문과 동일): 1백만개 경로를 저장할 수 있는 큰 재생 버퍼. 많은 RAM 필요\n",
        "\n",
        "replay_buffer_observer = replay_buffer.add_batch # 재생 버퍼에 경로를 저장할 옵저버"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7VPqFgcOElG"
      },
      "source": [
        "Create a simple custom observer that counts and displays the number of times it is called (except when it is passed a trajectory that represents the boundary between two episodes, as this does not count as a step):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoSKv-xfvcUA"
      },
      "source": [
        "호출될 때마다 카운터를 증가시키는 간단한 옵저버(스텝으로 카운트하지 않는 두 에피소드 사이의 경계를 나타낼 때는 제외)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DMR88YtOElG"
      },
      "source": [
        "# 100번 증가할 때마다 주어진 총게에 대한 진행 과정 표시\n",
        "class ShowProgress:\n",
        "    def __init__(self, total):\n",
        "        self.counter = 0\n",
        "        self.total = total\n",
        "    def __call__(self, trajectory):\n",
        "        if not trajectory.is_boundary():\n",
        "            self.counter += 1\n",
        "        if self.counter % 100 == 0:\n",
        "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I58nZoWhOElH"
      },
      "source": [
        "## Training Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bdKQnBKOElH"
      },
      "source": [
        "from tf_agents.metrics import tf_metrics\n",
        "\n",
        "train_metrics = [\n",
        "    tf_metrics.NumberOfEpisodes(), # 에피소드 횟수\n",
        "    tf_metrics.EnvironmentSteps(), # \n",
        "    tf_metrics.AverageReturnMetric(), # 에피소드 당 평균 대가: 각 에피소드의 할인되지 않는 보상의 합을 구하고, 처리한 모든 에피소드에 대해 이 합의 스트리밍 평균을 관리\n",
        "    tf_metrics.AverageEpisodeLengthMetric(), # 평균 에피소드 길이\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6t68Q1tOElH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "603f8cbc-5eea-4d49-8de4-029ddce1256e"
      },
      "source": [
        "train_metrics[0].result() # 각 지표의 값 확인"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int64, numpy=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojTh0GmbOElH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e179ec-b774-4f2a-d268-ad5fdc8f4e48"
      },
      "source": [
        "from tf_agents.eval.metric_utils import log_metrics\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "log_metrics(train_metrics) # 모든 지표를 로그에 기록"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl: \n",
            "\t\t NumberOfEpisodes = 0\n",
            "\t\t EnvironmentSteps = 0\n",
            "\t\t AverageReturn = 0.0\n",
            "\t\t AverageEpisodeLength = 0.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kJYRX9IOElH"
      },
      "source": [
        "## Collect Driver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNj7jye_zNXr"
      },
      "source": [
        "드라이버: 주어진 정책으로 환경을 탐색하고 경험을 수집하고 옵저버에 이를 전파하는 객체"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Ng0IrO1YZm"
      },
      "source": [
        "각 스텝에서 드라이버가 수행하는 작업\n",
        "\n",
        "\n",
        "*   드라이버는 현재 타임 스텝을 수집 정책에 전달. 수집 정책은 타임 스텝을 사용해 행동을 선택하고 행동을 포함한 행동 스텝을 반환\n",
        "*   그다음 드라이버는 행동을 환경에 전달하고 다음 타임 스텝을 반환받는다\n",
        "*   마지막으로 드라이버는 이 전이를 표현하는 경로 객체를 만들고 모든 옵저버에게 이를 전파\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7pbRk1xf4kh"
      },
      "source": [
        "\n",
        "\n",
        "1.   드라이버가 **배치 타임 스텝**을 정책에 전달<br>\n",
        "(관측의 배치, 스텝 타입의 배치, 보상의 배치, 할인의 배치를 포함한 타임 스텝 객체. 이 4개의 배치의 크기는 모두 같다)\n",
        "2.   드라이버는 이전 정책 상태의 배치도 전달\n",
        "3.   정책은 행동의 배치와 정책 상태의 배치를 담은 **배치 행동 스텝**을 반환\n",
        "4.   드라이버는 **배치 경로**를 만든다<br>\n",
        "(스텝 타입의 배치, 관측의 배치, 행돋의 배치, 보상의 배치와 각 경로의 속성에 대한 배치를 담은 경로. 모든 배치의 크기는 같다.)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgC-O86COElI"
      },
      "source": [
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "\n",
        "# (2015년 DQN 논문) 각 훈련 반복에서 스텝 4개에 대한 경험 수집\n",
        "collect_driver = DynamicStepDriver(\n",
        "    tf_env, # 환경\n",
        "    agent.collect_policy, # 에이전트의 수집 정책\n",
        "    observers=[replay_buffer_observer] + train_metrics, # 옵저버 리스트(재생 버퍼 옵저버와 훈련 지표)\n",
        "    num_steps=update_period) # collect 4 steps for each training iteration(실행할 스텝 횟수. 여기서는 4)\n",
        "\n",
        "# run() 메서드를 호출해서 실행할 수 있다\n",
        "# BUT 랜덤 정책으로 수집된 경험을 미리 재생 버퍼에 채워 놓는 것이 좋다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeJ6SL8wOElI"
      },
      "source": [
        "Collect the initial experiences, before training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRsfQCrhOElI"
      },
      "source": [
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "\n",
        "# (2015년 DQN 논문) 80,000개의 시뮬레이션 프레임으로\n",
        "# RandomTFPolicy 클래스를 사용해 20,000 스텝 동안 이 정책을 실행하는 두번쨰 드라이버\n",
        "# 재생 버퍼에 경험을 미리 채워 놓기 위해\n",
        "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
        "                                        tf_env.action_spec())\n",
        "init_driver = DynamicStepDriver(\n",
        "    tf_env,\n",
        "    initial_collect_policy,\n",
        "    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n",
        "    num_steps=20000) # <=> 80,000 ALE frames\n",
        "final_time_step, final_policy_state = init_driver.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7UjksjAOElK"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtbY_-KkOElI"
      },
      "source": [
        "Let's sample 2 sub-episodes, with 3 time steps each and display them:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NUNk9ZHOElI"
      },
      "source": [
        "**Note**: `replay_buffer.get_next()` is deprecated. We must use `replay_buffer.as_dataset(..., single_deterministic_pass=False)` instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYWtZ6ALOElI"
      },
      "source": [
        "tf.random.set_seed(9) # chosen to show an example of trajectory at the end of an episode\n",
        "\n",
        "#trajectories, buffer_info = replay_buffer.get_next( # get_next() is deprecated\n",
        "#    sample_batch_size=2, num_steps=3)\n",
        "\n",
        "trajectories, buffer_info = next(iter(replay_buffer.as_dataset(\n",
        "    sample_batch_size=2,\n",
        "    num_steps=3,\n",
        "    single_deterministic_pass=False)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQFKi5iSBkRl"
      },
      "source": [
        "trajectories 객체는 7개 필드로 이뤄진 네임드 튜플<br>\n",
        "경로가 2개 있고 각 스텝이 3개 있기 때문"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7qdkuptOElJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248160cf-6329-4d00-c5f1-9f2ba0646976"
      },
      "source": [
        "trajectories._fields "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('step_type',\n",
              " 'observation',\n",
              " 'action',\n",
              " 'policy_info',\n",
              " 'next_step_type',\n",
              " 'reward',\n",
              " 'discount')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myMhxaTWBkJs"
      },
      "source": [
        "observation필드 크기가 [2, 3, 84, 84, 4]<br>\n",
        ": 첫 두 차원의 크기가 2와 3인 텐서.<br>\n",
        "2개 경로에 대해 각 스텝이 3개 있고 각 스텝의 관측 크기는 84x84x4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukaVRIjPOElJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f591ce67-5fa6-4e93-af56-58f34a79dbfc"
      },
      "source": [
        "trajectories.observation.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 3, 84, 84, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVl1kpTUOElJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0493eae3-3df7-4d65-d908-8531ee4b6705"
      },
      "source": [
        "from tf_agents.trajectories.trajectory import to_transition\n",
        "\n",
        "# to_transition(): 경로(배치 time_step, 배치 action_step, 배치 next_time_step)를 담는 리스트로 변환\n",
        "time_steps, action_steps, next_time_steps = to_transition(trajectories)\n",
        "time_steps.observation.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 2, 84, 84, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib9egRPiOElK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec26932-413e-402f-c30d-fec35ebe5bab"
      },
      "source": [
        "trajectories.step_type.numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1],\n",
              "       [1, 1, 1]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd4txo4xOElK"
      },
      "source": [
        "plt.figure(figsize=(10, 6.8))\n",
        "for row in range(2):\n",
        "    for col in range(3):\n",
        "        plt.subplot(2, 3, row * 3 + col + 1)\n",
        "        plot_observation(trajectories.observation[row, col].numpy())\n",
        "plt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02)\n",
        "save_fig(\"sub_episodes_plot\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iOYluhXii60"
      },
      "source": [
        "Now let's create the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeNpoq1yOElK"
      },
      "source": [
        "#tf.data.Dataset: Data API의 장점(병렬화와 프리페칭)을 사용할 수 있다\n",
        "dataset = replay_buffer.as_dataset( # (2015년 DQN 논문)\n",
        "    sample_batch_size=64, # 훈련 스텝마다 64개 경로의 배치를 샘플링\n",
        "    num_steps=2, # 각 경로는 2개의 스텝을 포함(2개의 스텝 = 다음 스텝의 관측을 가진 1개의 완전한 전이)\n",
        "    num_parallel_calls=3).prefetch(3) # 이 데이터셋은 3개를 병렬로 처리하고 3개의 배치를 프리페치"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbrTysHROElL"
      },
      "source": [
        "## Main Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unl6WnyCOElL"
      },
      "source": [
        "Convert the main functions to TF Functions for better performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw6Jf80ROElL"
      },
      "source": [
        "from tf_agents.utils.common import function\n",
        "\n",
        "# 훈련 속도를 높이기 위해 주 함수를 텐서플로 함수로 변경\n",
        "# : tf.function()를 감싸고 실험적인 옵션을 추가한 tf_agents.utils.common.function() 함수를 사용\n",
        "collect_driver.run = function(collect_driver.run)\n",
        "agent.train = function(agent.train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpriTSYKOElL"
      },
      "source": [
        "def train_agent(n_iterations):\n",
        "    time_step = None\n",
        "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size) # 초기 상태(환경의 배치 크기 전달. 이 경우는 1): 이 정책은 상태가 없기 때문에 빈 튜플 반환(policy_state = () )\n",
        "\n",
        "    iterator = iter(dataset)\n",
        "    for iteration in range(n_iterations):\n",
        "        \n",
        "        # 수집 정책 실행, 4 스텝 동안 경험을 수집\n",
        "        time_step, policy_state = collect_driver.run(time_step, policy_state) \n",
        "\n",
        "        # 수집된 경로를 재생 버퍼와 지표에 전달\n",
        "        trajectories, buffer_info = next(iterator)\n",
        "\n",
        "        # 데이터셋에서 경로의 배치 하나를 샘플링하여 에이전트의 train() 메서드에 전달\n",
        "        train_loss = agent.train(trajectories) #train_loss 객체를 반환\n",
        "\n",
        "        # 반복 횟수와 훈련 손실을 출력\n",
        "        print(\"\\r{} loss:{:.5f}\".format(\n",
        "            iteration, train_loss.loss.numpy()), end=\"\")\n",
        "        # 1,000번 반복 마다 모든 지표를 로그에 기록\n",
        "        if iteration % 1000 == 0:\n",
        "            log_metrics(train_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xno-_0xROElL"
      },
      "source": [
        "Run the next cell to train the agent for 50,000 steps. Then look at its behavior by running the following cell. You can run these two cells as many times as you wish. The agent will keep improving! It will likely take over 200,000 iterations for the agent to become reasonably good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_3STpwUOElL"
      },
      "source": [
        "train_agent(n_iterations=50000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo1W3UVnOElM"
      },
      "source": [
        "frames = []\n",
        "def save_frames(trajectory):\n",
        "    global frames\n",
        "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
        "\n",
        "watch_driver = DynamicStepDriver(\n",
        "    tf_env,\n",
        "    agent.policy,\n",
        "    observers=[save_frames, ShowProgress(1000)],\n",
        "    num_steps=1000)\n",
        "final_time_step, final_policy_state = watch_driver.run()\n",
        "\n",
        "plot_animation(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c9fqhPXOElM"
      },
      "source": [
        "If you want to save an animated GIF to show off your agent to your friends, here's one way to do it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v7ZJQ8jOElM"
      },
      "source": [
        "import PIL\n",
        "\n",
        "image_path = os.path.join(\"images\", \"rl\", \"breakout.gif\")\n",
        "frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\n",
        "frame_images[0].save(image_path, format='GIF',\n",
        "                     append_images=frame_images[1:],\n",
        "                     save_all=True,\n",
        "                     duration=30,\n",
        "                     loop=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfQ6dWZuOElM"
      },
      "source": [
        "%%html\n",
        "<img src=\"images/rl/breakout.gif\" />"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}